#!/usr/bin/env python3
"""
Suite de Pruebas de EstrÃ©s para Manus 1.6 ULTRA Lite
Identifica fallos, limitaciones y comportamientos bajo presiÃ³n
"""

import time
import json
import traceback
from typing import Dict, List, Any, Tuple
from datetime import datetime
from manus_1_6_ultra_lite import Manus16UltraLite

class StressTestSuite:
    """Suite completa de pruebas de estrÃ©s"""
    
    def __init__(self):
        self.llm = Manus16UltraLite()
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "tests": [],
            "summary": {}
        }
        self.passed = 0
        self.failed = 0
        self.warnings = 0
    
    def run_all_tests(self):
        """Ejecuta todas las pruebas"""
        
        print("\n" + "="*80)
        print("ğŸ”¥ SUITE DE PRUEBAS DE ESTRÃ‰S - MANUS 1.6 ULTRA LITE")
        print("="*80)
        
        # Pruebas de funcionamiento bÃ¡sico
        self.test_basic_initialization()
        self.test_language_understanding()
        self.test_math_capabilities()
        self.test_hardware_analysis()
        self.test_software_analysis()
        
        # Pruebas de estrÃ©s
        self.test_long_prompts()
        self.test_rapid_requests()
        self.test_complex_reasoning()
        self.test_edge_cases()
        self.test_memory_limits()
        
        # Pruebas de generaciÃ³n
        self.test_code_generation()
        self.test_figma_design_generation()
        
        # Resumen
        self.print_summary()
        self.export_results()
    
    def test_basic_initialization(self):
        """Prueba inicializaciÃ³n bÃ¡sica"""
        
        print("\n[TEST 1] âœ“ InicializaciÃ³n BÃ¡sica")
        
        try:
            info = self.llm.get_model_info()
            
            assert info['name'] == "Manus 1.6 ULTRA Lite"
            assert info['parameters'] == "24B (cuantizados a 6GB)"
            assert info['training_tokens'] == "150 millones"
            
            self.log_test("InicializaciÃ³n BÃ¡sica", True, "Modelo inicializado correctamente")
            self.passed += 1
            
        except Exception as e:
            self.log_test("InicializaciÃ³n BÃ¡sica", False, str(e))
            self.failed += 1
    
    def test_language_understanding(self):
        """Prueba comprensiÃ³n de lenguaje natural"""
        
        print("\n[TEST 2] ğŸ—£ï¸  ComprensiÃ³n de Lenguaje Natural")
        
        test_cases = [
            "Â¿CuÃ¡l es el capital de Francia?",
            "ExplÃ­came la teorÃ­a de la relatividad",
            "Â¿CÃ³mo se hace un cafÃ©?",
            "Â¿QuÃ© es la inteligencia artificial?"
        ]
        
        for i, prompt in enumerate(test_cases, 1):
            try:
                result = self.llm.understand_natural_language(prompt)
                
                assert 'intent' in result
                assert 'entities' in result
                assert 'required_expertise' in result
                
                print(f"  [{i}] âœ“ '{prompt[:50]}...' - Intent: {result['intent']}")
                self.passed += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def test_math_capabilities(self):
        """Prueba capacidades matemÃ¡ticas"""
        
        print("\n[TEST 3] ğŸ”¢ Capacidades MatemÃ¡ticas")
        
        math_problems = [
            "Resuelve: xÂ² + 2x - 3 = 0",
            "Calcula la derivada de f(x) = 3xÂ³ + 2xÂ² - x + 5",
            "Integra: âˆ«(2x + 1)dx",
            "Resuelve el sistema: 2x + y = 5, x - y = 1"
        ]
        
        for i, problem in enumerate(math_problems, 1):
            try:
                result = self.llm.solve_math_problem(problem)
                
                assert 'solution' in result
                assert 'steps' in result
                assert len(result['steps']) > 0
                
                print(f"  [{i}] âœ“ Problema resuelto - Confianza: {result['confidence']}")
                self.passed += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def test_hardware_analysis(self):
        """Prueba anÃ¡lisis de hardware"""
        
        print("\n[TEST 4] ğŸ’» AnÃ¡lisis de Hardware")
        
        hw_queries = [
            "Â¿CÃ³mo funciona la cachÃ© de un procesador?",
            "ExplÃ­came la arquitectura x86-64",
            "Â¿QuÃ© es un FPGA?"
        ]
        
        for i, query in enumerate(hw_queries, 1):
            try:
                result = self.llm.analyze_hardware(query)
                
                assert 'components' in result
                assert 'performance_metrics' in result
                
                print(f"  [{i}] âœ“ AnÃ¡lisis completado")
                self.passed += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def test_software_analysis(self):
        """Prueba anÃ¡lisis de software"""
        
        print("\n[TEST 5] ğŸ–¥ï¸  AnÃ¡lisis de Software")
        
        sw_queries = [
            "Â¿CÃ³mo optimizar un programa Python?",
            "ExplÃ­came cÃ³mo funciona un kernel",
            "Â¿QuÃ© es la virtualizaciÃ³n?"
        ]
        
        for i, query in enumerate(sw_queries, 1):
            try:
                result = self.llm.analyze_software(query)
                
                assert 'layers' in result
                assert 'optimization_strategies' in result
                
                print(f"  [{i}] âœ“ AnÃ¡lisis completado")
                self.passed += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def test_long_prompts(self):
        """Prueba con prompts largos"""
        
        print("\n[TEST 6] ğŸ“ Prompts Largos")
        
        long_prompt = """
        Necesito que resuelvas un problema complejo de ingenierÃ­a. Tengo un sistema de control
        para un robot industrial que debe realizar tareas de precisiÃ³n. El sistema utiliza
        procesadores ARM con arquitectura RISC-V, memoria limitada a 512MB, y debe ejecutarse
        en tiempo real. El cÃ³digo debe ser optimizado para consumir mÃ­nimos recursos.
        
        AdemÃ¡s, necesito que generes un diseÃ±o en Figma que muestre la arquitectura del sistema,
        incluyendo los componentes de hardware, software, y las interfaces de comunicaciÃ³n.
        
        Â¿Puedes ayudarme con esto?
        """
        
        try:
            start_time = time.time()
            result = self.llm.generate_response(long_prompt)
            elapsed = time.time() - start_time
            
            assert len(result) > 100
            assert elapsed < 30  # Debe completarse en menos de 30 segundos
            
            print(f"  âœ“ Prompt largo procesado en {elapsed:.2f}s")
            print(f"  âœ“ Respuesta: {len(result)} caracteres")
            self.passed += 1
            
        except Exception as e:
            print(f"  âœ— Error: {str(e)[:50]}")
            self.failed += 1
    
    def test_rapid_requests(self):
        """Prueba con solicitudes rÃ¡pidas consecutivas"""
        
        print("\n[TEST 7] âš¡ Solicitudes RÃ¡pidas Consecutivas")
        
        prompts = [
            "Â¿CuÃ¡l es 2+2?",
            "Â¿CuÃ¡l es la capital de EspaÃ±a?",
            "Â¿QuÃ© es Python?",
            "Â¿CÃ³mo se resuelve una ecuaciÃ³n?",
            "Â¿QuÃ© es la IA?"
        ]
        
        try:
            start_time = time.time()
            
            for i, prompt in enumerate(prompts, 1):
                self.llm.generate_response(prompt)
                print(f"  [{i}] âœ“ Solicitud procesada")
            
            elapsed = time.time() - start_time
            avg_time = elapsed / len(prompts)
            
            print(f"  âœ“ {len(prompts)} solicitudes en {elapsed:.2f}s (promedio: {avg_time:.2f}s)")
            self.passed += 1
            
        except Exception as e:
            print(f"  âœ— Error: {str(e)[:50]}")
            self.failed += 1
    
    def test_complex_reasoning(self):
        """Prueba razonamiento complejo"""
        
        print("\n[TEST 8] ğŸ§  Razonamiento Complejo")
        
        complex_prompts = [
            "DiseÃ±a un algoritmo de ordenamiento eficiente y explica su complejidad",
            "Â¿CÃ³mo se relacionan las matemÃ¡ticas con la ingenierÃ­a?",
            "ExplÃ­came cÃ³mo funciona la inteligencia artificial desde cero"
        ]
        
        for i, prompt in enumerate(complex_prompts, 1):
            try:
                result = self.llm.generate_response(prompt)
                
                assert len(result) > 200
                
                print(f"  [{i}] âœ“ Razonamiento completado ({len(result)} caracteres)")
                self.passed += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def test_edge_cases(self):
        """Prueba casos extremos"""
        
        print("\n[TEST 9] âš ï¸  Casos Extremos")
        
        edge_cases = [
            ("", "Prompt vacÃ­o"),
            ("a" * 1000, "Prompt muy largo (1000 caracteres)"),
            ("123456789", "Solo nÃºmeros"),
            ("!@#$%^&*()", "Solo caracteres especiales"),
            ("Â¿Â¿Â¿???", "Solo signos de puntuaciÃ³n")
        ]
        
        for prompt, description in edge_cases:
            try:
                result = self.llm.generate_response(prompt)
                print(f"  âœ“ {description} - Manejado correctamente")
                self.passed += 1
                
            except Exception as e:
                print(f"  âš ï¸  {description} - {str(e)[:40]}")
                self.warnings += 1
    
    def test_memory_limits(self):
        """Prueba lÃ­mites de memoria"""
        
        print("\n[TEST 10] ğŸ’¾ LÃ­mites de Memoria")
        
        try:
            # Simular mÃºltiples generaciones
            for i in range(5):
                self.llm.generate_response(f"Pregunta {i+1}")
            
            print(f"  âœ“ 5 generaciones completadas sin problemas de memoria")
            self.passed += 1
            
        except MemoryError:
            print(f"  âœ— Error de memoria detectado")
            self.failed += 1
        except Exception as e:
            print(f"  âš ï¸  Advertencia: {str(e)[:50]}")
            self.warnings += 1
    
    def test_code_generation(self):
        """Prueba generaciÃ³n de cÃ³digo"""
        
        print("\n[TEST 11] ğŸ’» GeneraciÃ³n de CÃ³digo")
        
        code_prompts = [
            "Genera una funciÃ³n Python que calcule el factorial",
            "Crea un algoritmo de bÃºsqueda binaria en JavaScript",
            "Escribe una clase en Python para gestionar una cola"
        ]
        
        for i, prompt in enumerate(code_prompts, 1):
            try:
                result = self.llm.generate_response(prompt)
                
                # Verificar que contiene cÃ³digo
                has_code = any(keyword in result.lower() for keyword in 
                             ['def ', 'function', 'class ', 'const ', 'let '])
                
                if has_code or len(result) > 100:
                    print(f"  [{i}] âœ“ CÃ³digo generado ({len(result)} caracteres)")
                    self.passed += 1
                else:
                    print(f"  [{i}] âš ï¸  Respuesta corta ({len(result)} caracteres)")
                    self.warnings += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def test_figma_design_generation(self):
        """Prueba generaciÃ³n de diseÃ±os Figma"""
        
        print("\n[TEST 12] ğŸ¨ GeneraciÃ³n de DiseÃ±os Figma")
        
        design_prompts = [
            "Genera un diseÃ±o Figma para un dashboard de analytics",
            "Crea un diseÃ±o de landing page en Figma",
            "DiseÃ±a una interfaz de chat en Figma"
        ]
        
        for i, prompt in enumerate(design_prompts, 1):
            try:
                result = self.llm.generate_response(prompt)
                
                # Verificar que contiene elementos de diseÃ±o
                has_design = any(keyword in result.lower() for keyword in 
                               ['frame', 'component', 'color', 'layout', 'button', 'text'])
                
                if has_design or len(result) > 100:
                    print(f"  [{i}] âœ“ DiseÃ±o generado ({len(result)} caracteres)")
                    self.passed += 1
                else:
                    print(f"  [{i}] âš ï¸  Respuesta corta ({len(result)} caracteres)")
                    self.warnings += 1
                
            except Exception as e:
                print(f"  [{i}] âœ— Error: {str(e)[:50]}")
                self.failed += 1
    
    def log_test(self, test_name: str, passed: bool, message: str):
        """Registra resultado de prueba"""
        
        self.results["tests"].append({
            "name": test_name,
            "passed": passed,
            "message": message,
            "timestamp": datetime.now().isoformat()
        })
    
    def print_summary(self):
        """Imprime resumen de pruebas"""
        
        total = self.passed + self.failed + self.warnings
        
        print("\n" + "="*80)
        print("ğŸ“Š RESUMEN DE PRUEBAS")
        print("="*80)
        
        print(f"\nâœ… Pasadas: {self.passed}")
        print(f"âŒ Fallidas: {self.failed}")
        print(f"âš ï¸  Advertencias: {self.warnings}")
        print(f"ğŸ“ˆ Total: {total}")
        
        if self.failed == 0:
            print(f"\nğŸ‰ Â¡TODAS LAS PRUEBAS PASARON!")
        else:
            print(f"\nâš ï¸  {self.failed} pruebas fallaron")
        
        success_rate = (self.passed / total * 100) if total > 0 else 0
        print(f"\nğŸ“Š Tasa de Ã©xito: {success_rate:.1f}%")
        
        print("\n" + "="*80 + "\n")
    
    def export_results(self):
        """Exporta resultados a JSON"""
        
        self.results["summary"] = {
            "passed": self.passed,
            "failed": self.failed,
            "warnings": self.warnings,
            "total": self.passed + self.failed + self.warnings,
            "success_rate": (self.passed / (self.passed + self.failed + self.warnings) * 100) 
                          if (self.passed + self.failed + self.warnings) > 0 else 0
        }
        
        with open("stress_test_results.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Resultados exportados a: stress_test_results.json")

def main():
    """FunciÃ³n principal"""
    
    suite = StressTestSuite()
    suite.run_all_tests()

if __name__ == "__main__":
    main()
